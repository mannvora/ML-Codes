{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1b8ac10b-db82-44d5-8687-23d9dc778079",
   "metadata": {},
   "source": [
    "Your Name: Mann Anil Vora\n",
    "Your ASU ID: 1231868809"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82745e9-05a0-42eb-acd5-8398ca7d586e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework Assignment 2 - A Probablistic Naive Bayes Classifier\n",
    "\n",
    "CSE 575 Section C Fall 2024 Luo\n",
    "\n",
    "## Description\n",
    "\n",
    "**This is an individual work.** The project focuses on a subset of the MNIST dataset containing images of digits \"0\" and \"1\". The project involves four tasks: feature extraction, parameter calculation, implementation of Na√Øve Bayes classifiers, and prediction of labels for the test data using the classifiers. Finally, calculating the accuracy of the predictions.\n",
    "\n",
    "## What packages are allowed / prohibited\n",
    "\n",
    "You ARE allowed to use fundamental math/stat operators in numpy and math, such as numpy.var, numpy.std, numpy.mean and etc.\n",
    "\n",
    "You are <font color=\"red\">**NOT allowed**</font> to use functions that directly return Gaussian-dsitribution PDF values or directly use a NB classifier, e.g. scipy.stat.norm, numpy.random.normal, the sklearn library as a whole, or the likes. If we find you use any of those in your source code, your submission will be desk-rejected (receiving a 0). \n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- (7 pts) Your source code in this **HW2.ipynb** that contains all the proper implementations\n",
    "- (3 pts) A one-page **pdf** report, excluding the cover page if you have one. **You will report all output values (from Step 2 and Step 3) for all the 3 cases given to you**, and record any difficulty or problem you have encountered during the process.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The ground truths for the given 3 cases will be revealed on Canvas **by Tuesday, Feb 13th**. During grading, we will further assess your program and see if it can pass 2 additional hidden test cases. \n",
    "\n",
    "The error range for the PDF values in Step 2 is +-0.2, and the error range for the accuracies in Step 3 is +-0.005 .\n",
    "\n",
    "## Deadline\n",
    "\n",
    "2359 hours on Tuesday, Feb 20th aka the Midterm 1 day. No late submissions will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea208e78-5da4-4457-ac55-a9e9ff1d189e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "\n",
    "case = 0 # can also be 0, 1, or 2\n",
    "\n",
    "## This is a local path to the folder in my computer. Change it as per the requirement!!\n",
    "\n",
    "\n",
    "## Already set to Test case 2 ## Change it as per the requirement\n",
    "Numpyfile0 = scipy.io.loadmat(f'/home/mannvora/Desktop/CSE-575-SML/Assignments/ASSIGNMENT2/data/train2/digit0_stu_train2.mat')\n",
    "Numpyfile1 = scipy.io.loadmat(f'/home/mannvora/Desktop/CSE-575-SML/Assignments/ASSIGNMENT2/data/train2/digit1_stu_train2.mat')\n",
    "Numpyfile2 = scipy.io.loadmat('/home/mannvora/Desktop/CSE-575-SML/Assignments/ASSIGNMENT2/data/test/digit0_testset.mat')\n",
    "Numpyfile3 = scipy.io.loadmat('/home/mannvora/Desktop/CSE-575-SML/Assignments/ASSIGNMENT2/data/test/digit1_testset.mat')\n",
    "\n",
    "train0 = Numpyfile0.get('target_img') # Training samples of digit 0\n",
    "train1 = Numpyfile1.get('target_img') # Training samples of digit 1\n",
    "\n",
    "test0 = Numpyfile2.get('target_img') # Test samples of digit 0\n",
    "test1 = Numpyfile3.get('target_img') # Test samples of digit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df55833-0235-49e2-ba98-b9a7375f73dc",
   "metadata": {},
   "source": [
    "# 1. Feature Extraction - Transforming the Raw Images into 2D\n",
    "\n",
    "The images of MNIST in its raw form are all 28x28 greyscale pixel values, or 1x784 if flattened. That is way too many dimensions! How can we find a way to represent the images but with far fewer dimensions?\n",
    "\n",
    "For this project, we will transform all image samples into 2D vectors. You need to first extract features from your original training sets **i.e. train0, train1**, by converting the original data arrays to 2D arrays. The two features out of any image x are defined as following: \n",
    "\n",
    "**Feature1 / x_f1:** The average brightness of each image (average all pixel values within a whole image array). \n",
    "\n",
    "**Feature2 / x_f2:** The standard deviation of the brightness of each image (standard deviation of all pixel brightness values within a whole image array). \n",
    "\n",
    "We assume that these two features are i.i.d and are sampled from Gaussian distributions with regard to all images. Below is a function template you may make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad14ce9-e919-4e9c-a4e4-b22198db5093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54.09566327  97.5900018 ]\n",
      " [ 52.67984694  96.57544794]\n",
      " [ 48.31377551  93.12555171]\n",
      " ...\n",
      " [ 31.06377551  75.10089142]\n",
      " [ 38.66836735  84.32684964]\n",
      " [ 62.8877551  103.05217201]]\n",
      "[[19.19387755 63.13439797]\n",
      " [15.95153061 56.93856515]\n",
      " [17.25510204 59.17016471]\n",
      " ...\n",
      " [21.45790816 65.21079058]\n",
      " [ 9.52423469 41.79007211]\n",
      " [13.57142857 52.35572986]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1 Template (you are free to modify it at will)\n",
    "\n",
    "#Function to extract the feactures \n",
    "def feat_extract(x):\n",
    "    assert x.shape == (28, 28)\n",
    "    \n",
    "    # Calculate average brightness\n",
    "    x_f1 = np.mean(x)\n",
    "    \n",
    "    # Calculate standard deviation of brightness\n",
    "    x_f2 = np.std(x)\n",
    "    \n",
    "    return x_f1, x_f2\n",
    "\n",
    "\n",
    "##Extracting the features for digit 0\n",
    "\n",
    "num_samples = train0.shape[0]\n",
    "features_train0 = np.zeros((num_samples, 2))  \n",
    "\n",
    "for i in range(num_samples):\n",
    "    x = train0[i]\n",
    "    features_train0[i] = feat_extract(x)\n",
    "\n",
    "print(features_train0)\n",
    "\n",
    "## Doing the same to extract the features for Digit1 \n",
    "\n",
    "num_samples = train1.shape[0]\n",
    "features_train1 = np.zeros((num_samples, 2)) \n",
    "\n",
    "for i in range(num_samples):\n",
    "    x = train1[i]\n",
    "    features_train1[i] = feat_extract(x)\n",
    "    \n",
    "print(features_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499a94b-8222-4ce5-a7b0-ceeffa056e41",
   "metadata": {},
   "source": [
    "# 2. The NB Likehood Parameters are Distributed as Gaussian PDFs\n",
    "\n",
    "You need to calculate all the parameters for our two-class Naive Bayes (NB) classifier respectively, based upon the 2-D featurized data you have obtained in Step 1. \n",
    "\n",
    "**Assuming the two priors P(Y = 0) = P(Y = 1) = 0.5**, we now need to figure out **4 sets of Gaussian PDFs** for the 4 likelihood parameters in order to make predictions. **Remember, you obtain the parameters only from the training sets**:  \n",
    "\n",
    "1. Mean of Feature1 for digit0 \n",
    "2. Variance of Feature1 for digit0 \n",
    "3. Mean of Feature2 for digit0 \n",
    "4. Variance of Feature2 for digit0 \n",
    "5. Mean of Feature1 for digit1\n",
    "6. Variance of Feature1 for digit1 \n",
    "7. Mean of Feature2 for digit1 \n",
    "8. Variance of Feature2 for digit1 \n",
    "\n",
    "**At the end of this step, you need to print out a list that contains these 8 values in the above order.**\n",
    "\n",
    "Hint: Double check the NB classifier formula, what exactly are the 4 likelihood parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2593a425-2c77-4815-adb0-87ff6d6c14cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.15345433673469, 113.58645671141707, 87.39560564946916, 100.33375184004153, 19.379585969387758, 31.234096625264932, 61.3637481389197, 82.25698654654198]\n"
     ]
    }
   ],
   "source": [
    "def gaussian_pdf(mu, var, x_f):\n",
    "    # Compute Gaussian PDF\n",
    "    exponent = -((x_f - mu)**2) / (2 * var)\n",
    "    pdf = (1 / np.sqrt(2 * np.pi * var)) * np.exp(exponent)\n",
    "    return pdf\n",
    "\n",
    "# Calculate the mean and variance for each feature for digit 0\n",
    "mean_f1_digit0 = np.mean(features_train0[:, 0])  # Feature 1 (average brightness)\n",
    "var_f1_digit0 = np.var(features_train0[:, 0])\n",
    "\n",
    "mean_f2_digit0 = np.mean(features_train0[:, 1])  # Feature 2 (standard deviation of brightness)\n",
    "var_f2_digit0 = np.var(features_train0[:, 1])\n",
    "\n",
    "# Calculate the mean and variance for each feature for digit 1\n",
    "mean_f1_digit1 = np.mean(features_train1[:, 0])  # Feature 1 (average brightness)\n",
    "var_f1_digit1 = np.var(features_train1[:, 0])\n",
    "\n",
    "mean_f2_digit1 = np.mean(features_train1[:, 1])  # Feature 2 (standard deviation of brightness)\n",
    "var_f2_digit1 = np.var(features_train1[:, 1])\n",
    "\n",
    "print([mean_f1_digit0, var_f1_digit0,\n",
    "       mean_f2_digit0, var_f2_digit0,\n",
    "       mean_f1_digit1, var_f1_digit1,\n",
    "       mean_f2_digit1, var_f2_digit1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb270c1-9071-4a5e-8d54-268731daa5ea",
   "metadata": {},
   "source": [
    "# 3. NB Predictions and Accuracies out of the Test Sets\n",
    "\n",
    "Once Step 1 and Step 2 are finished, you now have everything to generate prodictions for samples in our test sets. We have two test sets in this project: **test0** of all digit 0's, and **test1** of all digit 1's.\n",
    "\n",
    "Your prediction using a NB classifier is by comparing the two posteriors - P(Y=0|X) vs. P(Y=1|X). For a single test set image X in its raw form, you will obtain the two posteriors by doing \n",
    "\n",
    "1. Convert X into the same feature1-feature2 vectors with your feature extractor done in Step 1, then\n",
    "2. Use the 4 established Gaussian PDFs from Step 2 to obtain the two posteriors and make judgements.\n",
    "\n",
    "**At the end of this step, you need to print out a two-item list that contains the overall prediction accuracies for test0 and test1. The accuracy values should both be within 0 and 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264feed4-a3c9-4737-8aab-21dc08395696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9173469387755102, 0.9233480176211454]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Predictions\n",
    "correct0 = 0  # Counter for correct predictions in test0\n",
    "correct1 = 0  # Counter for correct predictions in test1\n",
    "\n",
    "# Predictions for test0 (digit 0)\n",
    "for x in test0:\n",
    "    x_f1, x_f2 = feat_extract(x)\n",
    "    \n",
    "    # Calculate posteriors for digit 0 and digit 1\n",
    "    posterior_digit0 = gaussian_pdf(mean_f1_digit0, var_f1_digit0, x_f1) * gaussian_pdf(mean_f2_digit0, var_f2_digit0, x_f2)\n",
    "    posterior_digit1 = gaussian_pdf(mean_f1_digit1, var_f1_digit1, x_f1) * gaussian_pdf(mean_f2_digit1, var_f2_digit1, x_f2)\n",
    "    \n",
    "    # Make prediction\n",
    "    if posterior_digit0 > posterior_digit1:\n",
    "        correct0 += 1\n",
    "\n",
    "# Accuracy for test0\n",
    "acc0 = correct0 / len(test0)\n",
    "\n",
    "# Predictions for test1 (digit 1)\n",
    "for x in test1:\n",
    "    x_f1, x_f2 = feat_extract(x)\n",
    "    \n",
    "    # Calculate posteriors for digit 0 and digit 1\n",
    "    posterior_digit0 = gaussian_pdf(mean_f1_digit0, var_f1_digit0, x_f1) *  gaussian_pdf(mean_f2_digit0, var_f2_digit0, x_f2)\n",
    "    posterior_digit1 = gaussian_pdf(mean_f1_digit1, var_f1_digit1, x_f1) *  gaussian_pdf(mean_f2_digit1, var_f2_digit1, x_f2)\n",
    "    \n",
    "    # Make prediction\n",
    "    if posterior_digit1 > posterior_digit0:\n",
    "        correct1 += 1\n",
    "\n",
    "# Accuracy for test1\n",
    "acc1 = correct1 / len(test1)\n",
    "\n",
    "# Print accuracies\n",
    "print([acc0, acc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702b7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
